{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "MIR 2.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyOUaKd1nlydCxig2So9FRrt",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Robak132/MIR/blob/main/Z2/pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1xku1ZdYIr2",
    "outputId": "2c22c6c0-5815-4735-a0b4-9f70ecabd338",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'genres.tar.gz'\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget http://opihi.cs.uvic.ca/sound/genres.tar.gz\n",
    "!tar -zxvf genres.tar.gz\n",
    "!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\n",
    "!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\n",
    "!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torch.utils import data\n",
    "from torchaudio_augmentations import (\n",
    "    RandomResizedCrop,\n",
    "    RandomApply,\n",
    "    PolarityInversion,\n",
    "    Noise,\n",
    "    Gain,\n",
    "    HighLowPass,\n",
    "    Delay,\n",
    "    PitchShift,\n",
    "    Reverb,\n",
    "    Compose,\n",
    ")\n",
    "\n",
    "\n",
    "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "\n",
    "class GTZANDataset(data.Dataset):\n",
    "    def __init__(self, data_path, split, num_samples, num_chunks, is_augmentation):\n",
    "        self.data_path =  data_path if data_path else ''\n",
    "        self.split = split\n",
    "        self.num_samples = num_samples\n",
    "        self.num_chunks = num_chunks\n",
    "        self.is_augmentation = is_augmentation\n",
    "        self.genres = GTZAN_GENRES\n",
    "        self._get_song_list()\n",
    "        if is_augmentation:\n",
    "            self._get_augmentations()\n",
    "\n",
    "    def _get_song_list(self):\n",
    "        list_filename = os.path.join(self.data_path, '%s_filtered.txt' % self.split)\n",
    "        with open(list_filename) as f:\n",
    "            lines = f.readlines()\n",
    "        self.song_list = [line.strip() for line in lines]\n",
    "\n",
    "    def _get_augmentations(self):\n",
    "        transforms = [\n",
    "            RandomResizedCrop(n_samples=self.num_samples),\n",
    "            RandomApply([PolarityInversion()], p=0.8),\n",
    "            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n",
    "            RandomApply([Gain()], p=0.2),\n",
    "            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n",
    "            RandomApply([Delay(sample_rate=22050)], p=0.5),\n",
    "            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n",
    "            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n",
    "        ]\n",
    "        self.augmentation = Compose(transforms=transforms)\n",
    "\n",
    "    def _adjust_audio_length(self, wav):\n",
    "        if self.split == 'train':\n",
    "            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n",
    "            wav = wav[random_index : random_index + self.num_samples]\n",
    "        else:\n",
    "            hop = (len(wav) - self.num_samples) // self.num_chunks\n",
    "            wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)])\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.song_list[index]\n",
    "\n",
    "        # get genre\n",
    "        genre_name = line.split('/')[0]\n",
    "        genre_index = self.genres.index(genre_name)\n",
    "\n",
    "        # get audio\n",
    "        audio_filename = os.path.join(self.data_path, 'genres', line)\n",
    "        wav, fs = sf.read(audio_filename)\n",
    "\n",
    "        # adjust audio length\n",
    "        wav = self._adjust_audio_length(wav).astype('float32')\n",
    "\n",
    "        # data augmentation\n",
    "        if self.is_augmentation:\n",
    "            wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0)).squeeze(0).numpy()\n",
    "\n",
    "        return wav, genre_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_list)\n",
    "\n",
    "def get_dataloader(data_path=None, \n",
    "                   split='train', \n",
    "                   num_samples=22050 * 29, \n",
    "                   num_chunks=1, \n",
    "                   batch_size=16, \n",
    "                   num_workers=0, \n",
    "                   is_augmentation=False):\n",
    "    is_shuffle = True if (split == 'train') else False\n",
    "    batch_size = batch_size if (split == 'train') else (batch_size // num_chunks)\n",
    "    data_loader = data.DataLoader(dataset=GTZANDataset(data_path, \n",
    "                                                       split, \n",
    "                                                       num_samples, \n",
    "                                                       num_chunks, \n",
    "                                                       is_augmentation),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=is_shuffle,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader"
   ],
   "metadata": {
    "id": "dQ9e8HxhYLqg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soundfile'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msoundfile\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msf\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchaudio_augmentations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      8\u001B[0m     RandomResizedCrop,\n\u001B[0;32m      9\u001B[0m     RandomApply,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     Compose,\n\u001B[0;32m     18\u001B[0m )\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'soundfile'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = get_dataloader(split='train', is_augmentation=True)\n",
    "iter_train_loader = iter(train_loader)\n",
    "train_wav, train_genre = next(iter_train_loader)\n",
    "\n",
    "valid_loader = get_dataloader(split='valid')\n",
    "test_loader = get_dataloader(split='test')\n",
    "iter_test_loader = iter(test_loader)\n",
    "test_wav, test_genre = next(iter_test_loader)\n",
    "print('training data shape: %s' % str(train_wav.shape))\n",
    "print('validation/test data shape: %s' % str(test_wav.shape))\n",
    "print(train_genre)"
   ],
   "metadata": {
    "id": "1YBtnhKmYLti",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = CNN().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "valid_losses = []\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    # Train\n",
    "    cnn.train()\n",
    "    for (wav, genre_index) in train_loader:\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        # Forward\n",
    "        out = cnn(wav)\n",
    "        loss = loss_function(out, genre_index)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(losses)))\n",
    "\n",
    "    # Validation\n",
    "    cnn.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    losses = []\n",
    "    for wav, genre_index in valid_loader:\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        # reshape and aggregate chunk-level predictions\n",
    "        b, c, t = wav.size()\n",
    "        logits = cnn(wav.view(-1, t))\n",
    "        logits = logits.view(b, c, -1).mean(dim=1)\n",
    "        loss = loss_function(logits, genre_index)\n",
    "        losses.append(loss.item())\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "        # append labels and predictions\n",
    "        y_true.extend(genre_index.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    valid_loss = np.mean(losses)\n",
    "    print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy: %.4f' % (epoch+1, num_epochs, valid_loss, accuracy))\n",
    "\n",
    "    # Save model\n",
    "    valid_losses.append(valid_loss.item())\n",
    "    if np.argmin(valid_losses) == epoch:\n",
    "        print('Saving the best model at %d epochs!' % epoch)\n",
    "        torch.save(cnn.state_dict(), 'best_model.ckpt')"
   ],
   "metadata": {
    "id": "EufhIGB_YLwq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the best model\n",
    "S = torch.load('best_model.ckpt')\n",
    "cnn.load_state_dict(S)\n",
    "print('loaded!')\n",
    "\n",
    "# Run evaluation\n",
    "cnn.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for wav, genre_index in test_loader:\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        # reshape and aggregate chunk-level predictions\n",
    "        b, c, t = wav.size()\n",
    "        logits = cnn(wav.view(-1, t))\n",
    "        logits = logits.view(b, c, -1).mean(dim=1)\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "        # append labels and predictions\n",
    "        y_true.extend(genre_index.tolist())\n",
    "        y_pred.extend(pred.tolist())"
   ],
   "metadata": {
    "id": "u3DxOL5rYLzn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, xticklabels=GTZAN_GENRES, yticklabels=GTZAN_GENRES, cmap='YlGnBu')\n",
    "print('Accuracy: %.4f' % accuracy)"
   ],
   "metadata": {
    "id": "GPgREnukYL2V",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}